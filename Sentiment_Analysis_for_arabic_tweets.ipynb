{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis for arabic tweets",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamzeh1995/MachineLearning/blob/master/Sentiment_Analysis_for_arabic_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvdUXUnb03F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scbMqkJa07FO",
        "colab_type": "code",
        "outputId": "61847a22-35d6-4fe3-d226-21047edea5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCl3OkRc1J4_",
        "colab_type": "code",
        "outputId": "9d151abd-2141-49b0-8cb6-39cc3478f131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "with open(\"/content/drive/My Drive/Tweets/Positive/positive1.txt\", \"r\") as f:\n",
        "    d = f.readlines()\n",
        "    f.seek(0)\n",
        "    for i in d:\n",
        "        print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿حقا\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql8CvyPv1j2P",
        "colab_type": "code",
        "outputId": "63cae232-a51c-4e55-c6be-2e75df51e6ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import pandas\n",
        "\n",
        "tweet = []\n",
        "label = []\n",
        "\n",
        "\n",
        "for x in range(1,1000):\n",
        "    try:\n",
        "      tweet_text_positive = open('/content/drive/My Drive/Tweets/Positive/positive{}.txt'.format(x), mode=\"r\", encoding=\"utf-8\")\n",
        "      tweet.append(tweet_text_positive.read().replace('\\n',''))\n",
        "      label.append('POSITIVE')\n",
        "    except:\n",
        "      print('Error in positive at: {}'.format(x))\n",
        "    \n",
        "    try:\n",
        "      tweet_text_negative = open('/content/drive/My Drive/Tweets/Negative/negative{}.txt'.format(x), mode=\"r\", encoding=\"utf-8\")\n",
        "      pos = tweet_text_negative.read()\n",
        "      tweet.append(pos.replace('\\n',''))\n",
        "      label.append('NEGATIVE')\n",
        "    except:\n",
        "      print('Error in negative at: {}'.format(x))\n",
        "    \n",
        "\n",
        "\n",
        "           \n",
        "tweets = pandas.DataFrame({'label': label , 'tweet' :tweet })"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in negative at: 103\n",
            "Error in negative at: 116\n",
            "Error in negative at: 176\n",
            "Error in negative at: 178\n",
            "Error in negative at: 180\n",
            "Error in negative at: 184\n",
            "Error in negative at: 186\n",
            "Error in negative at: 189\n",
            "Error in negative at: 191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHulqoY23Qv0",
        "colab_type": "code",
        "outputId": "7a906447-b758-4898-f867-0987e5ff8985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "source": [
        "tweets.head(20)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>﻿حقا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿و الله حرام و الله موتوه لشعب الاردني من وين ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>الحل الوحيد هو القرب من الله وذكره  (الا بذكر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>من وين انجيب حرام عليكم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>وهذه من اكبر المشاكل التي تؤرق الشباب في هذه ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿مش شايف العالم مش ملاقيه الخبز . كل واحد بشتغ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>الموت مع الناس رحمه..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿و الله حرام الناس مش لاقيه توكل حتى ترفعو الب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>اي صح و حاليا فاقده الاثنين</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>الله لا يشبعكو يا ظلمة انشاء الله كلو بتلقو با...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>معلومه ممتازه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>بس الله لا يسامحكم و تلقوها بصحتكم ان شاء الله.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>فعلا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿و الله انكو ما بتخافو الله</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>هستوكم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿منكو لله احنا لاقيين حق الخبز لحتى ترفعو البن...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>احسن علاج هذا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>الله اكبر على كل ظالم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>صح جدا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿البنزين يجي من السعوديه بأقل الاسعار و الحكوم...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label                                              tweet\n",
              "0   POSITIVE                                               ﻿حقا\n",
              "1   NEGATIVE  ﻿و الله حرام و الله موتوه لشعب الاردني من وين ...\n",
              "2   POSITIVE   الحل الوحيد هو القرب من الله وذكره  (الا بذكر...\n",
              "3   NEGATIVE                            من وين انجيب حرام عليكم\n",
              "4   POSITIVE   وهذه من اكبر المشاكل التي تؤرق الشباب في هذه ...\n",
              "5   NEGATIVE  ﻿مش شايف العالم مش ملاقيه الخبز . كل واحد بشتغ...\n",
              "6   POSITIVE                              الموت مع الناس رحمه..\n",
              "7   NEGATIVE  ﻿و الله حرام الناس مش لاقيه توكل حتى ترفعو الب...\n",
              "8   POSITIVE                        اي صح و حاليا فاقده الاثنين\n",
              "9   NEGATIVE  الله لا يشبعكو يا ظلمة انشاء الله كلو بتلقو با...\n",
              "10  POSITIVE                                      معلومه ممتازه\n",
              "11  NEGATIVE    بس الله لا يسامحكم و تلقوها بصحتكم ان شاء الله.\n",
              "12  POSITIVE                                               فعلا\n",
              "13  NEGATIVE                        ﻿و الله انكو ما بتخافو الله\n",
              "14  POSITIVE                                            هستوكم \n",
              "15  NEGATIVE  ﻿منكو لله احنا لاقيين حق الخبز لحتى ترفعو البن...\n",
              "16  POSITIVE                                      احسن علاج هذا\n",
              "17  NEGATIVE                              الله اكبر على كل ظالم\n",
              "18  POSITIVE                                             صح جدا\n",
              "19  NEGATIVE  ﻿البنزين يجي من السعوديه بأقل الاسعار و الحكوم..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctBewmI33Sdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the Tweets DataFrame to a CSV file and save it for future use\n",
        "\n",
        "tweets.to_csv('/content/tweets.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWh80xIf8LOz",
        "colab_type": "code",
        "outputId": "f9d8b293-0e84-453a-ec24-29a2bd93426e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Explore the DataSet \n",
        "\n",
        "POSITIVE = tweets.query('label == \"POSITIVE\"')\n",
        "NEGATIVE = tweets.query('label == \"NEGATIVE\"')\n",
        "\n",
        "print('Total POSITIVE:\\n\\n')\n",
        "\n",
        "print(POSITIVE.count())\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "print('POSITIVE Sample: ' + POSITIVE['tweet'][70])\n",
        "\n",
        "\n",
        "print('Total NEGATIVE:\\n\\n')\n",
        "\n",
        "print(NEGATIVE.count())\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "print('NEGATIVE Sample: ' + NEGATIVE['tweet'][1984])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total POSITIVE:\n",
            "\n",
            "\n",
            "label    999\n",
            "tweet    999\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "\n",
            "POSITIVE Sample: طبعا واقع\n",
            "Total NEGATIVE:\n",
            "\n",
            "\n",
            "label    990\n",
            "tweet    990\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "\n",
            "NEGATIVE Sample: ﻿ بكتب ع اوراق الشجر يقول موهبتي وانا صغير الله يسامح اللي كاذب عليه\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rih4_q2M8v5o",
        "colab_type": "code",
        "outputId": "425cf04a-5765-414b-d0cc-91ee79ce009f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Install PyArabic for Arabic text processing \n",
        "\n",
        "!pip install pyarabic\n",
        "import pyarabic.araby as araby\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.6/dist-packages (0.6.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE4KexNF82IO",
        "colab_type": "code",
        "outputId": "8bb126c5-9478-4375-96f0-997bce4c0da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Import NLTK to get Arabic stop words \n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3nUDG3z87vB",
        "colab_type": "code",
        "outputId": "8fb016ab-4ff0-4eab-e5ed-c379a0490f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "\n",
        "print(arb_stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'أن', 'بي', 'أنتما', 'عليك', 'ولو', 'فيه', 'لسن', 'هل', 'ألا', 'أقل', 'عند', 'ته', 'هناك', 'مع', 'آي', 'إنما', 'تلكما', 'ذلكن', 'لي', 'حيثما', 'لئن', 'اللتين', 'من', 'ليستا', 'بما', 'تينك', 'سوف', 'ذلكم', 'بس', 'مذ', 'ولكن', 'نعم', 'كيت', 'فلا', 'إليك', 'الذين', 'غير', 'لهم', 'هذا', 'لك', 'هكذا', 'تلك', 'إلا', 'ليسوا', 'هنالك', 'هذي', 'ممن', 'قد', 'في', 'لهما', 'ذينك', 'دون', 'أنتم', 'ذه', 'كأن', 'اللذان', 'هن', 'ذان', 'كلتا', 'أولاء', 'وإذا', 'لولا', 'وإذ', 'ذواتا', 'تي', 'ليسا', 'أينما', 'أيها', 'إنا', 'بماذا', 'اللذين', 'أنت', 'خلا', 'ذو', 'ومن', 'وإن', 'بمن', 'لكيلا', 'مما', 'هاهنا', 'لن', 'منه', 'لسنا', 'أوه', 'ليت', 'حاشا', 'هما', 'هنا', 'لعل', 'أو', 'هذان', 'هذين', 'عليه', 'حتى', 'بهم', 'لنا', 'كأنما', 'أين', 'هذه', 'نحن', 'ثم', 'لهن', 'كليهما', 'لو', 'إي', 'كلما', 'نحو', 'هاته', 'أنتن', 'عن', 'إذن', 'إذما', 'لستم', 'بعض', 'له', 'وما', 'هيهات', 'هاك', 'متى', 'كيفما', 'على', 'بك', 'فإذا', 'والذين', 'ماذا', 'إلى', 'بلى', 'إليكن', 'أنى', 'لكما', 'كأي', 'إليكم', 'فيها', 'كل', 'مهما', 'ذاك', 'عما', 'لدى', 'لوما', 'هي', 'اللاتي', 'كي', 'كيف', 'ذواتي', 'مه', 'أم', 'لست', 'إذا', 'لستما', 'لها', 'اللتان', 'كم', 'حيث', 'كذلك', 'ذوا', 'الذي', 'كذا', 'لا', 'ليست', 'ها', 'بين', 'هؤلاء', 'التي', 'ذي', 'كأين', 'هاتين', 'بنا', 'ليس', 'أف', 'ثمة', 'لستن', 'كليكما', 'لكي', 'اللائي', 'حين', 'لم', 'أنا', 'عدا', 'إنه', 'بخ', 'شتان', 'ذلكما', 'بل', 'إيه', 'بهن', 'فمن', 'لما', 'به', 'فيم', 'تلكم', 'وهو', 'ذلك', 'إن', 'فيما', 'بعد', 'ذين', 'منذ', 'هاتي', 'بها', 'أولئك', 'آه', 'عسى', 'إما', 'هلا', 'هم', 'بكم', 'هيت', 'بهما', 'اللواتي', 'عل', 'أي', 'اللتيا', 'كلاهما', 'أما', 'منها', 'ولا', 'إليكما', 'ذات', 'سوى', 'إذ', 'كلا', 'يا', 'هاتان', 'ما', 'ذانك', 'بكما', 'هو', 'كما', 'تين', 'بكن', 'ريث', 'هيا', 'لكنما', 'لاسيما', 'ذا', 'والذي', 'بيد', 'آها', 'حبذا', 'أكثر', 'فإن', 'لكم', 'لكن'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgXBEkYx9KS_",
        "colab_type": "code",
        "outputId": "5d5fc7b1-ddcf-4336-f674-00cab4fb3c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import string \n",
        "\n",
        "\n",
        "ignore_alif_lam = [\"الله\", \"اللهم\", \"الم\", \"اليم\", \"الف\", \"الفه\", \"اليف\", \"البس\", \"الباس\",\n",
        "                   \"التبس\",\"التباس\", \"القاب\",\"السن\", \"العاب\",\"العوبه\",\"المعيه\", \"الق\", \"القى\", \"القاء\", \"التقاء\", \"التقى\", \"المع\", \"الفى\",\"الفت\", \"الفات\", \"التفت\", \"التفات\"]\n",
        "\n",
        "ignore_lam = [\"لله\"]\n",
        "\n",
        "punct_ = ['.', ',', '؟', ':', '\"', '،', '؛', '\\'', ')', '(', '<', '>']\n",
        "\n",
        "\n",
        "def clean_token(token):\n",
        "  token = araby.strip_tashkeel(token)\n",
        "  token = araby.strip_tatweel(token)\n",
        "  token = token.replace('أ','ا')\n",
        "  token = token.replace('آ','ا')\n",
        "  token = token.replace('إ','ا')\n",
        "  token = token.replace('ة','ه')\n",
        "\n",
        "  if token not in ignore_alif_lam and token not in arb_stopwords:\n",
        "    if token.startswith(\"ال\"):\n",
        "      token = token[2:]\n",
        "    elif token.startswith(\"فال\"):\n",
        "      token = token[3:]\n",
        "    elif token.startswith(\"بال\"):\n",
        "      token = token[3:]\n",
        "    elif token.startswith(\"كال\"):\n",
        "      token = token[3:]\n",
        "    elif token.startswith(\"لل\") and token not in ignore_lam:\n",
        "      token = token[2:]\n",
        "\n",
        "  token = ''.join([char for char in token if char not in string.punctuation and char not in punct_])\n",
        "\n",
        "  return token\n",
        "\n",
        "\n",
        "\n",
        "print(clean_token('البيت'))\n",
        "\n",
        "normalized_arb_stopwords = [clean_token(word) for word in arb_stopwords]\n",
        "\n",
        "normalized_arb_stopwords.append('\\xa0')\n",
        "normalized_arb_stopwords.append('\\ufeff')\n",
        "\n",
        "\n",
        "\n",
        "print(normalized_arb_stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "بيت\n",
            "['ان', 'بي', 'انتما', 'عليك', 'ولو', 'فيه', 'لسن', 'هل', 'ا', 'اقل', 'عند', 'ته', 'هناك', 'مع', 'اي', 'انما', 'تلكما', 'ذلكن', 'لي', 'حيثما', 'لئن', 'اللتين', 'من', 'ليستا', 'بما', 'تينك', 'سوف', 'ذلكم', 'بس', 'مذ', 'ولكن', 'نعم', 'كيت', 'فلا', 'يك', 'الذين', 'غير', 'لهم', 'هذا', 'لك', 'هكذا', 'تلك', 'ا', 'ليسوا', 'هنالك', 'هذي', 'ممن', 'قد', 'في', 'لهما', 'ذينك', 'دون', 'انتم', 'ذه', 'كان', 'اللذان', 'هن', 'ذان', 'كلتا', 'اولاء', 'واذا', 'لولا', 'واذ', 'ذواتا', 'تي', 'ليسا', 'اينما', 'ايها', 'انا', 'بماذا', 'اللذين', 'انت', 'خلا', 'ذو', 'ومن', 'وان', 'بمن', 'لكيلا', 'مما', 'هاهنا', 'لن', 'منه', 'لسنا', 'اوه', 'ليت', 'حاشا', 'هما', 'هنا', 'لعل', 'او', 'هذان', 'هذين', 'عليه', 'حتى', 'بهم', 'لنا', 'كانما', 'اين', 'هذه', 'نحن', 'ثم', 'لهن', 'كليهما', 'لو', 'اي', 'كلما', 'نحو', 'هاته', 'انتن', 'عن', 'اذن', 'اذما', 'لستم', 'بعض', 'له', 'وما', 'هيهات', 'هاك', 'متى', 'كيفما', 'على', 'بك', 'فاذا', 'والذين', 'ماذا', 'ى', 'بلى', 'يكن', 'انى', 'لكما', 'كاي', 'يكم', 'فيها', 'كل', 'مهما', 'ذاك', 'عما', 'لدى', 'لوما', 'هي', 'اللاتي', 'كي', 'كيف', 'ذواتي', 'مه', 'ام', 'لست', 'اذا', 'لستما', 'لها', 'اللتان', 'كم', 'حيث', 'كذلك', 'ذوا', 'الذي', 'كذا', 'لا', 'ليست', 'ها', 'بين', 'هؤلاء', 'التي', 'ذي', 'كاين', 'هاتين', 'بنا', 'ليس', 'اف', 'ثمه', 'لستن', 'كليكما', 'لكي', 'اللائي', 'حين', 'لم', 'انا', 'عدا', 'انه', 'بخ', 'شتان', 'ذلكما', 'بل', 'ايه', 'بهن', 'فمن', 'لما', 'به', 'فيم', 'تلكم', 'وهو', 'ذلك', 'ان', 'فيما', 'بعد', 'ذين', 'منذ', 'هاتي', 'بها', 'اولئك', 'اه', 'عسى', 'اما', 'هلا', 'هم', 'بكم', 'هيت', 'بهما', 'اللواتي', 'عل', 'اي', 'اللتيا', 'كلاهما', 'اما', 'منها', 'ولا', 'يكما', 'ذات', 'سوى', 'اذ', 'كلا', 'يا', 'هاتان', 'ما', 'ذانك', 'بكما', 'هو', 'كما', 'تين', 'بكن', 'ريث', 'هيا', 'لكنما', 'لاسيما', 'ذا', 'والذي', 'بيد', 'اها', 'حبذا', 'اكثر', 'فان', 'لكم', 'لكن', '\\xa0', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pepg1Sp9OTtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def tokenize_and_clean(text):\n",
        "    #tokens = nltk.word_tokenize(text)\n",
        "    tokens = araby.tokenize(text)\n",
        "    tokens = [clean_token(word) for word in tokens if word not in normalized_arb_stopwords]\n",
        "    return tokens\n",
        "\n",
        "tweets['tokenized_tweet'] = tweets['tweet'].apply(lambda x: tokenize_and_clean(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpWyr0YHPX4O",
        "colab_type": "code",
        "outputId": "4dd43d01-cfb9-4992-c221-4de304716d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "def count_punct(text):\n",
        "    count = sum([1 for char in text if char in string.punctuation or char in punct_])\n",
        "    #print(count)\n",
        "    try:\n",
        "      return round(count/(len(text) - text.count(\" \")), 3)*100\n",
        "    except:\n",
        "      return 0.0\n",
        "\n",
        "#print(count_punct('Hi, how are you doing?'))\n",
        "\n",
        "tweets['body_len'] = tweets['tweet'].apply(lambda x: len(x) - x.count(\" \"))\n",
        "\n",
        "tweets['punct%'] = tweets['tweet'].apply(lambda x: count_punct(x))\n",
        "\n",
        "tweets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tokenized_tweet</th>\n",
              "      <th>body_len</th>\n",
              "      <th>punct%</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>﻿حقا</td>\n",
              "      <td>[حقا]</td>\n",
              "      <td>4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿و الله حرام و الله موتوه لشعب الاردني من وين ...</td>\n",
              "      <td>[و, الله, حرام, و, الله, موتوه, لشعب, اردني, و...</td>\n",
              "      <td>98</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>الحل الوحيد هو القرب من الله وذكره  (الا بذكر...</td>\n",
              "      <td>[حل, وحيد, قرب, الله, وذكره, , ا, بذكر, الله, ...</td>\n",
              "      <td>65</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>من وين انجيب حرام عليكم</td>\n",
              "      <td>[وين, انجيب, حرام, عليكم]</td>\n",
              "      <td>19</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>وهذه من اكبر المشاكل التي تؤرق الشباب في هذه ...</td>\n",
              "      <td>[وهذه, اكبر, مشاكل, تؤرق, شباب, ايام]</td>\n",
              "      <td>43</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1984</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>﻿ بكتب ع اوراق الشجر يقول موهبتي وانا صغير الل...</td>\n",
              "      <td>[بكتب, ع, اوراق, شجر, يقول, موهبتي, وانا, صغير...</td>\n",
              "      <td>55</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>اللهم اجعلنا من اللذين يصلحون فى الارض ولا يفسدون</td>\n",
              "      <td>[اللهم, اجعلنا, يصلحون, فى, ارض, يفسدون]</td>\n",
              "      <td>41</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1986</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>هذا اللي بالنص من لجنة الحكم مفكر نفسو وديع ا...</td>\n",
              "      <td>[لي, نص, لجنه, حكم, مفكر, نفسو, وديع, صافي, و,...</td>\n",
              "      <td>52</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>اللهم إجعلنا ممن يأخذون بالأسباب</td>\n",
              "      <td>[اللهم, اجعلنا, ياخذون, اسباب]</td>\n",
              "      <td>28</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>مسخره والله</td>\n",
              "      <td>[مسخره, والله]</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1989 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label  ... punct%\n",
              "0     POSITIVE  ...    0.0\n",
              "1     NEGATIVE  ...    0.0\n",
              "2     POSITIVE  ...    3.1\n",
              "3     NEGATIVE  ...    0.0\n",
              "4     POSITIVE  ...    0.0\n",
              "...        ...  ...    ...\n",
              "1984  NEGATIVE  ...    0.0\n",
              "1985  POSITIVE  ...    0.0\n",
              "1986  NEGATIVE  ...    0.0\n",
              "1987  POSITIVE  ...    0.0\n",
              "1988  NEGATIVE  ...    0.0\n",
              "\n",
              "[1989 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-mLkfxlQfyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets[['tweet', 'body_len', 'punct%']], tweets['label'], test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xbRg0EVQ0rm",
        "colab_type": "code",
        "outputId": "69389d9a-7d4f-4304-c32d-4b00f6aad178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer=tokenize_and_clean)\n",
        "tfidf_vect_fit = tfidf_vect.fit(X_train['tweet'])\n",
        "\n",
        "tfidf_train = tfidf_vect_fit.transform(X_train['tweet'])\n",
        "\n",
        "tfidf_test = tfidf_vect_fit.transform(X_test['tweet'])\n",
        "\n",
        "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
        "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
        "\n",
        "\n",
        "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
        "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
        "\n",
        "X_train_vect.head()\n",
        "print(X_train_vect)\n",
        "print(X_test_vect)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      body_len  punct%         0    1    2  ...  5156  5157  5158  5159  5160\n",
            "0           27     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "1           25     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "2           27     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "3           22     4.5  0.200457  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "4           20     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "...        ...     ...       ...  ...  ...  ...   ...   ...   ...   ...   ...\n",
            "1586        24     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "1587        23     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "1588        32     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "1589       158     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "1590        17     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[1591 rows x 5163 columns]\n",
            "     body_len  punct%         0    1    2  ...  5156  5157  5158  5159  5160\n",
            "0          20     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "1           6     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "2          18     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "3          19    31.6  0.263045  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "4          22     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "..        ...     ...       ...  ...  ...  ...   ...   ...   ...   ...   ...\n",
            "393        84     2.4  0.123524  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "394        13     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "395         8     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "396        20     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "397        34     0.0  0.000000  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0\n",
            "\n",
            "[398 rows x 5163 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWBJjwlzaErN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AbttkxtaK9f",
        "colab_type": "code",
        "outputId": "d945ba7d-8a4a-4370-b8f0-4b6f0098b00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
        "\n",
        "start = time.time()\n",
        "rf_model = rf.fit(X_train_vect, y_train)\n",
        "end = time.time()\n",
        "fit_time = (end - start)\n",
        "\n",
        "start = time.time()\n",
        "y_pred = rf_model.predict(X_test_vect)\n",
        "end = time.time()\n",
        "pred_time = (end - start)\n",
        "\n",
        "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='POSITIVE', average='binary')\n",
        "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
        "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))\n",
        "\n",
        "#Fit time: 3.933 / Predict time: 0.118 ---- Precision: 0.87 / Recall: 0.703 / Accuracy: 0.789 ------ Without Removing Alif Lam \n",
        "#Fit time: 3.55 / Predict time: 0.126 ---- Precision: 0.909 / Recall: 0.74 / Accuracy: 0.819 ------- After Removing Alif Lam "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit time: 3.587 / Predict time: 0.121 ---- Precision: 0.86 / Recall: 0.743 / Accuracy: 0.804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5nVUNxgaRSt",
        "colab_type": "code",
        "outputId": "46518310-284c-438e-b61d-e26fbcc8e088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "def sentiment_detector(text):\n",
        "  d = {'tweet': text, 'body_len': len(text) - text.count(\" \"), 'punct%': count_punct(text)}\n",
        "  df= pd.DataFrame({k: [v] for k, v in d.items()})\n",
        "  #print(df)\n",
        "  \n",
        "  tfidf_vect_p = TfidfVectorizer(analyzer=tokenize_and_clean)\n",
        "\n",
        "  tfidf_vect_fit_p = tfidf_vect_p.fit(X_train['tweet'])\n",
        "\n",
        "  tfidf_predict = tfidf_vect_fit_p.transform(df['tweet'])\n",
        "  \n",
        "  predict_vect = pd.concat([df[['body_len', 'punct%']].reset_index(drop=True), pd.DataFrame(tfidf_predict.toarray())], axis=1)\n",
        "\n",
        "  #print(predict_vect.head())\n",
        "\n",
        "  rf_predict = rf_model.predict(predict_vect)\n",
        "\n",
        "  #gb_predict = gb_model.predict(predict_vect)\n",
        "  print('Random Forest Prediction : ' + rf_predict)\n",
        "  #print('Gradient Boost Prediction : ' + gb_predict)\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_sentence = 'مسخرة'\n",
        "sentiment_detector(test_sentence)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Random Forest Prediction : NEGATIVE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0WATJMaVoJ",
        "colab_type": "code",
        "outputId": "fcb4b0e7-ac19-4e9f-f115-bcf4f8e7aff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_sentence = 'hi'\n",
        "sentiment_detector(test_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Random Forest Prediction : POSITIVE']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}